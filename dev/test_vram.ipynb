{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\n",
    "            'Dreamy psychedelic rock, vintage guitar, laidback, hypnotic bass, rhythmic drums',\n",
    "            'Experimental electronica, trippy synths, ambient, progressive structure, dynamic bassline',\n",
    "            'Jazzy hip hop, smooth saxophone, mellow, chill beat, melodic bass',\n",
    "            'Euphoric trance, pulsating synth, uplifting, energetic drums, harmonious pads',\n",
    "            'Retro country, twangy guitar, laidback, soothing harmonica, rhythmic bass',\n",
    "            'Vintage ska, upbeat horns, energetic, syncopated guitar, bouncing bass',\n",
    "            'Sultry soul, emotive vocals, chill, rhythmic organ, mellow bassline',\n",
    "            'Retro country, twangy guitar, laidback, soothing harmonica, rhythmic bass',\n",
    "            'Vintage ska, upbeat horns, energetic, syncopated guitar, bouncing bass',\n",
    "            'Sultry soul, emotive vocals, chill, rhythmic organ, mellow bassline',\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 22.19 GiB total capacity; 20.29 GiB already allocated; 5.38 MiB free; 20.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_size \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m MusicGen\u001b[39m.\u001b[39;49mget_pretrained(model_size)\n\u001b[1;32m     10\u001b[0m duration \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mset_generation_params(\n\u001b[1;32m     12\u001b[0m             use_sampling\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m             top_k\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m             cfg_coef\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m     17\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/musicgen.py:96\u001b[0m, in \u001b[0;36mget_pretrained\u001b[0;34m(name, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m HF_MODEL_CHECKPOINTS_MAP:\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(name) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(name):\n\u001b[1;32m     93\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m is not a valid checkpoint name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChoose one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(HF_MODEL_CHECKPOINTS_MAP\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 96\u001b[0m         )\n\u001b[1;32m     98\u001b[0m cache_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mMUSICGEN_ROOT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m compression_model \u001b[39m=\u001b[39m load_compression_model(name, device\u001b[39m=\u001b[39mdevice, cache_dir\u001b[39m=\u001b[39mcache_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/loaders.py:61\u001b[0m, in \u001b[0;36mload_lm_model\u001b[0;34m(file_or_url, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39melif\u001b[39;00m file_or_url_or_id\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload_state_dict_from_url(file_or_url_or_id, map_location\u001b[39m=\u001b[39mdevice, check_hash\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 61\u001b[0m \u001b[39melif\u001b[39;00m file_or_url_or_id \u001b[39min\u001b[39;00m HF_MODEL_CHECKPOINTS_MAP:\n\u001b[1;32m     62\u001b[0m     \u001b[39massert\u001b[39;00m filename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mfilename needs to be defined if using HF checkpoints\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     repo_id \u001b[39m=\u001b[39m HF_MODEL_CHECKPOINTS_MAP[file_or_url_or_id]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/builders.py:119\u001b[0m, in \u001b[0;36mget_lm_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    105\u001b[0m         codebooks_pattern_cfg \u001b[39m=\u001b[39m omegaconf\u001b[39m.\u001b[39mOmegaConf\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m    106\u001b[0m             {\u001b[39m'\u001b[39m\u001b[39mmodeling\u001b[39m\u001b[39m'\u001b[39m: q_modeling, \u001b[39m'\u001b[39m\u001b[39mdelay\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mdelays\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(n_q))}}\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     pattern_provider \u001b[39m=\u001b[39m get_codebooks_pattern_provider(n_q, codebooks_pattern_cfg)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m LMModel(\n\u001b[1;32m    110\u001b[0m         pattern_provider\u001b[39m=\u001b[39;49mpattern_provider,\n\u001b[1;32m    111\u001b[0m         condition_provider\u001b[39m=\u001b[39;49mcondition_provider,\n\u001b[1;32m    112\u001b[0m         fuser\u001b[39m=\u001b[39;49mfuser,\n\u001b[1;32m    113\u001b[0m         cfg_dropout\u001b[39m=\u001b[39;49mcfg_prob,\n\u001b[1;32m    114\u001b[0m         cfg_coef\u001b[39m=\u001b[39;49mcfg_coef,\n\u001b[1;32m    115\u001b[0m         attribute_dropout\u001b[39m=\u001b[39;49mattribute_dropout,\n\u001b[1;32m    116\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(torch, cfg\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m    117\u001b[0m         device\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    118\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m--> 119\u001b[0m     )\u001b[39m.\u001b[39;49mto(cfg\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnexpected LM model \u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mlm_model\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 22.19 GiB total capacity; 20.29 GiB already allocated; 5.38 MiB free; 20.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "\n",
    "model_size = 'large'\n",
    "model = MusicGen.get_pretrained(model_size)\n",
    "\n",
    "\n",
    "duration = 5\n",
    "model.set_generation_params(\n",
    "            use_sampling=True,\n",
    "            top_k=250,\n",
    "            top_p=0.2,\n",
    "            duration=duration,\n",
    "            cfg_coef=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m prompts_to_use \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(prompts, n_prompts)\n\u001b[1;32m      4\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m generated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(prompts_to_use, progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m overlap\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m      9\u001b[0m last_moment \u001b[39m=\u001b[39m generated[:, :, \u001b[39m-\u001b[39moverlap\u001b[39m*\u001b[39mmodel\u001b[39m.\u001b[39msample_rate:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/musicgen.py:147\u001b[0m, in \u001b[0;36mMusicGen.generate\u001b[0;34m(self, descriptions, progress)\u001b[0m\n\u001b[1;32m    145\u001b[0m attributes, prompt_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_tokens_and_attributes(descriptions, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    146\u001b[0m \u001b[39massert\u001b[39;00m prompt_tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_tokens(attributes, prompt_tokens, progress)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/musicgen.py:282\u001b[0m, in \u001b[0;36mMusicGen._generate_tokens\u001b[0;34m(self, attributes, prompt_tokens, progress)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m# generate by sampling from LM\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast:\n\u001b[0;32m--> 282\u001b[0m     gen_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm\u001b[39m.\u001b[39;49mgenerate(prompt_tokens, attributes, callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_params)\n\u001b[1;32m    284\u001b[0m \u001b[39m# generate audio\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39massert\u001b[39;00m gen_tokens\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/lm.py:489\u001b[0m, in \u001b[0;36mLMModel.generate\u001b[0;34m(self, prompt, conditions, num_samples, max_gen_len, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg, remove_prompts, check, callback)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (curr_sequence \u001b[39m==\u001b[39m unknown_token)\u001b[39m.\u001b[39many()\n\u001b[1;32m    488\u001b[0m \u001b[39m# sample next token from the model, next token shape is [B, K, 1]\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m next_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_next_token(\n\u001b[1;32m    490\u001b[0m     curr_sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p,\n\u001b[1;32m    491\u001b[0m     cfg_coef\u001b[39m=\u001b[39;49mcfg_coef)\n\u001b[1;32m    492\u001b[0m \u001b[39m# ensure the tokens that should be masked are properly set to special_token_id\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39m# as the model never output special_token_id\u001b[39;00m\n\u001b[1;32m    494\u001b[0m valid_mask \u001b[39m=\u001b[39m mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, offset:offset\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mexpand(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/models/lm.py:369\u001b[0m, in \u001b[0;36mLMModel._sample_next_token\u001b[0;34m(self, sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p, cfg_coef)\u001b[0m\n\u001b[1;32m    367\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logits \u001b[39m/\u001b[39m temp, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m top_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m--> 369\u001b[0m     next_token \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49msample_top_p(probs, p\u001b[39m=\u001b[39;49mtop_p)\n\u001b[1;32m    370\u001b[0m \u001b[39melif\u001b[39;00m top_k \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    371\u001b[0m     next_token \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39msample_top_k(probs, k\u001b[39m=\u001b[39mtop_k)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiocraft/utils/utils.py:125\u001b[0m, in \u001b[0;36msample_top_p\u001b[0;34m(probs, p)\u001b[0m\n\u001b[1;32m    123\u001b[0m probs_sum \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcumsum(probs_sort, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m mask \u001b[39m=\u001b[39m probs_sum \u001b[39m-\u001b[39m probs_sort \u001b[39m>\u001b[39m p\n\u001b[0;32m--> 125\u001b[0m probs_sort \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m~\u001b[39;49mmask)\u001b[39m.\u001b[39;49mfloat(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    126\u001b[0m probs_sort\u001b[39m.\u001b[39mdiv_(probs_sort\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    127\u001b[0m next_token \u001b[39m=\u001b[39m multinomial(probs_sort, num_samples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "n_prompts = 9\n",
    "prompts_to_use = random.sample(prompts, n_prompts)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "generated = model.generate(prompts_to_use, progress=True)\n",
    "\n",
    "overlap=5\n",
    "last_moment = generated[:, :, -overlap*model.sample_rate:]\n",
    "\n",
    "continuation_section = model.generate_continuation(last_moment, model.sample_rate, descriptions=prompts_to_use, progress=True)\n",
    "\n",
    "full_continuation_tracks = torch.cat([generated[:, :, :-overlap*model.sample_rate], continuation_section], 2)\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "print(f\"took {elapsed_time} seconds to generate {n_prompts} prompts of {duration} seconds each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Dreamy_psychedelic_rock,_vintage_guitar,_laidback,_hypnotic_bass,_rhythmic_drums_0.wav happening with proba (a bit of clipping is okay): 0.0031284091528505087 maximum scale:  1.9001662731170654\n",
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Jazzy_hip_hop,_smooth_saxophone,_mellow,_chill_beat,_melodic_bass_2.wav happening with proba (a bit of clipping is okay): 0.010600567795336246 maximum scale:  1.6139987707138062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/AiRadio/scratch_output/Dreamy_psychedelic_rock,_vintage_guitar,_laidback,_hypnotic_bass,_rhythmic_drums_0.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Experimental_electronica,_trippy_synths,_ambient,_progressive_structure,_dynamic_bassline_1.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Jazzy_hip_hop,_smooth_saxophone,_mellow,_chill_beat,_melodic_bass_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Euphoric_trance,_pulsating_synth,_uplifting,_energetic_drums,_harmonious_pads_3.wav happening with proba (a bit of clipping is okay): 7.784090848872438e-05 maximum scale:  1.3691251277923584\n",
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Retro_country,_twangy_guitar,_laidback,_soothing_harmonica,_rhythmic_bass_4.wav happening with proba (a bit of clipping is okay): 9.14772754185833e-05 maximum scale:  1.2240946292877197\n",
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Vintage_ska,_upbeat_horns,_energetic,_syncopated_guitar,_bouncing_bass_5.wav happening with proba (a bit of clipping is okay): 0.00012329545279499143 maximum scale:  1.270970106124878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/AiRadio/scratch_output/Euphoric_trance,_pulsating_synth,_uplifting,_energetic_drums,_harmonious_pads_3.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Retro_country,_twangy_guitar,_laidback,_soothing_harmonica,_rhythmic_bass_4.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Vintage_ska,_upbeat_horns,_energetic,_syncopated_guitar,_bouncing_bass_5.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Sultry_soul,_emotive_vocals,_chill,_rhythmic_organ,_mellow_bassline_6.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Retro_country,_twangy_guitar,_laidback,_soothing_harmonica,_rhythmic_bass_7.wav\n",
      "Saved to /home/ubuntu/AiRadio/scratch_output/Vintage_ska,_upbeat_horns,_energetic,_syncopated_guitar,_bouncing_bass_8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Sultry_soul,_emotive_vocals,_chill,_rhythmic_organ,_mellow_bassline_6.wav happening with proba (a bit of clipping is okay): 3.409090822970029e-06 maximum scale:  1.1553694009780884\n",
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Retro_country,_twangy_guitar,_laidback,_soothing_harmonica,_rhythmic_bass_7.wav happening with proba (a bit of clipping is okay): 2.6136363885598257e-05 maximum scale:  1.1651315689086914\n",
      "CLIPPING /home/ubuntu/AiRadio/scratch_output/Vintage_ska,_upbeat_horns,_energetic,_syncopated_guitar,_bouncing_bass_8.wav happening with proba (a bit of clipping is okay): 3.409090822970029e-06 maximum scale:  1.0084209442138672\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.data.audio import audio_write\n",
    "import os\n",
    "\n",
    "\n",
    "clips = full_continuation_tracks\n",
    "\n",
    "\n",
    "for i in range(clips.shape[0]):\n",
    "\n",
    "    file_name = prompts[i].replace(\" \", \"_\") + f\"_{i}.wav\"\n",
    "\n",
    "    clip = clips.detach().cpu().float()[i]\n",
    "\n",
    "    full_location = os.path.join(\"/home/ubuntu/AiRadio/scratch_output\", file_name)\n",
    "    with open(full_location, 'wb') as file:\n",
    "        audio_write(\n",
    "            file.name, clip, model.sample_rate, strategy=\"loudness\",\n",
    "            loudness_headroom_db=16, add_suffix=False)\n",
    "        print(f'Saved to {file.name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
